# 통계학습 (statistical learning)
데이터에 대한 이해를 위한 방대한 도구 집합
***
## 통계학습이란?
- 설명변수들의 집합 X와 반응변수 Y 사이에 상관관계가 있다는 가정은 Y=f(X)+e로 나타낼 수 있다
- f는 X가 Y에 대해 제공하는 체계적인 정보를 나타낸다
- 통계학습은 f를 추정하는 일련의 기법들을 말한다
- f를 추정하는 두 가지 주요한 이유는 **예측**과 **추론**이다

## 예측 (prediction)
- Y에 대한 예측인 Y^의 정확성은 축소가능 오차(reducible error)와 축소불가능 오차(irreducible error)에 달려 있다
- 축소가능 오차 : 일반적으로 f^은 f를 완벽하게 추정하지 못하며, 이러한 부정확성으로 인해 발생하는 오차
- 축소불가능 오차 : 심지어 f를 완벽하게 추정해도 예측값은 여전히 어떤 오차를 가짐. Y 또한 e의 함수이고, 정의에 의해 e는 X를 사용하여 예측할 수 없기 때문이다.

## 추론 (inference)
- 예측을 넘어 X들의 함수로서 Y가 어떻게 변하는지 둘 사이의 관계를 이해하고자 한다 = 추론
- 예시 : 제품의 가격(X)을 변경하는 것은 판매(Y)에 어떤 영향을 미칠 것인가?

## 모수적 방법 (parametric methods)
- 통계학습의 한 기법, 다음의 2단계로 된 모델 기반의 기법
- (1) 먼저 f의 함수 형태에 대해 가정한다. f가 선형인 경우, 완전한 f(X) 함수 대신 계수만 추정하면 된다
- (2) 모델 선택 후 훈련 데이터를 사용하여 모델을 적합하거나 훈련시키는 절차가 필요하다
- 파라미터들을 추정하는 것이 함수를 찾아내는 것보다 쉽지만, 실제 f와 맞지 않을 것이라는 잠재적 단점이 있다
- 유연한 모델을 선택하려면 추정해야 하는 파라미터 수도 많아지고, 과적합의 문제가 있을 수 있다

## 비모수적 방법 (non-parametric methods)
- f 함수에 대해 명시적인 가정을 하지 않고, 데이터에 가까워지는 f의 추정을 얻으려고 한다
- f에 대해 어떠한 가정도 하지 않아, 더 넓은 범위의 f 형태에 정확하게 적합될 가능성이 있다
- 반면 아주 많은 수의 관측치가 필요하다 (모수적 방법에서 요구하는 것보다 훨씬 더) 

## 지도학습 (supervised learning)
- 지도학습 : 하나 이상의 입력변수를 기반으로 출력변수를 예측하거나 추정하는 통계적 모델을 만드는 것
- 선형회귀, 로지스틱 회귀, GAM, 부스팅, 서포트 벡터 머신

## 비지도학습 (unsupervised learning)
- 비지도학습 : 출력변수 없이 입력변수로 자료의 상관관계와 구조를 파악하는 것
- 클러스터링 : 설명변수들을 기반으로 관측치들이 상대적으로 구별되는 그룹에 속하는지 확인하는 것

## 변수의 종류
- 양적 변수 : 수치 값을 취하는 것 (나이, 키, 수입, 집값, 주식 가격)
- 질적 변수(범주형 변수) : K개의 다른 클래스 또는 카테고리 중의 하나를 값으로 가짐 (성별 등)
- 보통 양적 변수는 회귀 문제, 질적 변수는 분류 문제
- 그러나 코딩만 적절히 하면 설명변수 유형에 상관 없이 다양한 통계적 모델링이 가능하다
***

## 적합의 품질 추정
- 통계학습 방법 성능의 평가로, 예측값이 실제값에 얼마나 가까운지를 수량화하는 것이 필요하다
- 평균제곱오차(MSE) : 실제값에서 예측값을 빼고 제곱한 것을 다 더해서 평균 낸 것
- 예측값과 실제값이 아주 가까울수록 MSE는 아주 작을 것이다 
- 우리의 관심은 검정 데이터에 적용할 때 얻는 예측의 정확도, 즉 검정 MSE
- 훈련 데이터를 사용해 적합한 모델의 MSE는 훈련 MSE, 검정 MSE와 확연히 다를 수 있다
- 통계학습 방법의 유연성이 증가함에 따라 훈련 MSE는 단조감소, 검정 MSE는 U 모양을 보인다
- 훈련 MSE는 작지만, 검정 MSE는 큰 결과를 제공할 때 데이터를 과적합(overfitting)한다고 한다
- 과적합은 함수 f의 실제 성질에 의한 것이 아니라 단순한 우연에 의한 것도 패턴화할 수 있기 때문
- 검정 MSE가 최소가 되는 지점을 실제로 추정하는 데 사용되는 기법 = 교차검증(cross-validation)

## 편향-분산 절충 (bias variance trade-off)
- 검정 MSE = f^(x0)의 분산 + f^(x0)의 제곱편향 + 오차항 e의 분산
- f^(x0) : 추정한 모델 f^에 검정 데이터를 넣은 값
- 기대검정오차를 최소화하기 위해서는 낮은 분산과 낮은 편향을 동시에 달성해야
- 분산 : 다른 훈련자료를 사용하여 추정하는 경우 f^이 변동되는 정도
- 편향 : 실제 문제를 훨씬 단순한 모델로 근사시킴으로 인해 발생되는 오차
- 유연성이 높은 방법을 사용할수록 분산이 증가하고 편향은 감소
- 편향과 분산, 둘 다 낮은 방법을 찾는 것이 중요하다

## 분류 설정
- 분류 문제에서도 편향-분산 절충 개념이 동일하게 적용
- 훈련 MSE 대신 훈련 오차율(잘못 분류된 비율), 검정 MSE 대신 검정 오차율 사용
- 베이즈 분류기 : 주어진 X에 대한 Y의 조건부 분포를 알 경우 조건부 확률, P(Y=j|X=x0)
- KNN 분류기 : X에 대한 Y의 조건부 분포를 추정하여 가장 높은 추정확률을 가지는 클래스로 분류
